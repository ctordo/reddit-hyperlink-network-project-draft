---
layout: project
title: Reddit Hyperlink Network Project
sections:
  - id: intro
    title: Introduction
  - id: preprocessing
    title: Data preprocessing
  - id: metric
    title: Toxicity metrics
  - id: resultats
    title: Résultats
  - id: visualisation
    title: Visualisation
  - id: discussion
    title: Discussion
  - id: limites
    title: Limites
  - id: conclusion
    title: Conclusion
---

<section id="cover" class="cover-page">
  <div class="title">
    Epidemiology of Toxicity in the Reddit Hyperlink Network
  </div>
  
  <div class="subtitle">
    As an unusual patient - Reddit - turns to the ADA laboratory with its medical concerns, 
    only one team of doctors specialized in data-driven diagnosis anwsers the call: <strong>AdAstra</strong>.
    
    This notebook contains all the research and results carried out by the team as part of this medical consultation.
  </div>
  
  <table class="doctors-table">
    <tr>
      <td>
        <img src="{{ '/assets/img/photodoc1.jpg' | relative_url }}" alt="Aksel Acar" class="doctor-photo">
        <div class="doctor-name">Aksel Acar</div>
      </td>
      <td>
        <img src="{{ '/assets/img/photodoc2.jpg' | relative_url }}" alt="Emilien Coudurier" class="doctor-photo">
        <div class="doctor-name">Emilien Coudurier</div>
      </td>
      <td>
        <img src="{{ '/assets/img/photodoc3.jpg' | relative_url }}" alt="Nathan Tabet" class="doctor-photo">
        <div class="doctor-name">Nathan Tabet</div>
      </td>
      <td>
        <img src="{{ '/assets/img/photodoc4.jpg' | relative_url }}" alt="Cyprien Tordo" class="doctor-photo">
        <div class="doctor-name">Cyprien Tordo</div>
      </td>
      <td>
        <img src="{{ '/assets/img/photodoc5.jpg' | relative_url }}" alt="Cedric Zanou" class="doctor-photo">
        <div class="doctor-name">Cedric Zanou</div>
      </td>
    </tr>
  </table>
  
  <div class="footer">
    <div class="class">EPFL CS401 - Applied Data Analysis</div>
    <div class="date">December 19, 2025</div>
  </div>
</section>


<!-- #################################################################################################################################### -->
<!-- INTRO -->
<!-- #################################################################################################################################### -->

<section id="intro" class="section-block">  
  <h1>Introduction</h1>

  <h3>Patient description</h3>

  <img src="{{ '/assets/img/redditlogo.png' | relative_url }}" alt="Reddit Logo" class="center medium">
  
  <p>
    <strongorange>Reddit</strongorange> is a large, community-driven social media platform <strong>structured around user-created forums known as subreddits.</strong> 
    Each subreddit centers on a specific topic (ranging from news, politics, and science to niche hobbies and pop-culture 
    interests) and functions as its own semi-autonomous community with distinct norms, moderators, and posting cultures. 
    Interactions on Reddit occur primarily through posts and comments, and one subreddit can reference another by including 
    a hyperlink within the text or title of a post. These cross-subreddit links form what is known as the <strongorange>Reddit Hyperlink 
    Network</strongorange>: a web of directed connections that reveals how information, discussions, and community attention flow across the 
    platform.
  </p>
  
  <h3>Reason of consultation</h3>
  
  <p>
    Reddit comes in for consultation with a recurring concern: <strong>toxicity</strong>. Toxicity refers to patterns of communication 
    or behavior that generate hostility, disrespect, or harm among participants. Toxic interactions can take many forms, including 
    personal insults, harassment, hate speech, persistent negativity, or attempts to provoke conflict. Such behaviors undermine the 
    quality of discourse, discourage constructive participation, and can gradually erode the health of online communities.
  </p>

  <p>
  The architecture of Reddit makes it particularly susceptible to toxic dynamics: users can remain pseudonymous with minimal barriers 
  to participation, which lowers the social cost of engaging in hostile behavior. Subreddits vary widely in moderation strictness and 
  community norms, meaning that toxicity can flourish unevenly, concentrate in certain spaces, or spill over into others through 
  cross-community interactions. These characteristics make Reddit an especially compelling platform for studying how toxic behavior 
  emerges, propagates, and interacts with the structural features of the network.

  </p>
  
  <h3>Objective of consultation</h3>
  
  <p>
    State research questions here:
  </p>

  <ul>
    <li>Research question</li>
    <li>Research question</li>
    <li>Research question</li>
  </ul>
  
  
  <h3>Methodology overview</h3>

  <ol>
    <li>Exploring, preprocessing and cleaning data</li>
    <li>Designing a metric to detect and quantify toxicity</li>
    <li>Classifying different toxicity propagation styles</li>
    <li>Parallelizing toxicity styles with subreddits' thematics</li>
    <li>In-depth case studies</li>
  </ol>
  
  <p>
    Through these sequential approaches, the project seeks to build a richer understanding of toxicity on Reddit by 
    connecting behavioral patterns with the structural and thematic dimensions of the platform’s hyperlink network.
  </p>

</section>

<!-- #################################################################################################################################### -->
<!-- PREPROCESSING -->
<!-- #################################################################################################################################### -->

<section id="preprocessing" class="section-block">
  <h1>Data Preprocessing</h1>
  
  <h2>Title</h2>

  <h3>Subtitle</h3>

  <h4>Subsubtitle</h4>

  <p>
    AAA
  </p>
  
  <h3>Subtitle</h3>
  <p>
    BBB
  </p>
  
  <h3>Subtitle</h3>
  <p>
    CCC
  </p>
</section>

<!-- #################################################################################################################################### -->
<!-- METRICS -->
<!-- #################################################################################################################################### -->

<section id="metric" class="section-block">
  <h1>Toxicity measurements</h1>
  
  <p>
  Our first fundamental preliminary step is to <strong>calculate a reliable and accurate metric of toxicity</strong> of each subreddit's post.
  </p>

  <h2>Principal toxicity metrics</h2>
  
  <h3>Toxicity score</h3>

  <h4>What do we mean by "toxicity"?</h4>

  
  <p>
  We define toxicity as patterns of communication or behavior that create a hostile, disrespectful, or harmful environment for 
  participants. This can include insults, harassment, hate speech, excessive negativity, or attempts to provoke conflict. 
  Toxicity reduces constructive discussion, discourages participation, and can damage the overall health and cohesion of the community.
  </p>
  <p>
  This definition aligns with standard computational social science literature, where toxicity requires both negative emotional valence, 
  and linguistic markers of aggression or harmful intent. Reddit, being a large, anonymous, and heterogeneous platform, often exhibits these 
  patterns, making it an appropriate setting for a linguistic toxicity metric.
  </p>
  
  <div class="box-warning">
    <code>TOXICITY_SCORE</code>: Continuous score that measures the toxicity intensity in a subreddit post
  </div>

  <p>
    <strong>So, how will we we detect and quantify toxicity?</strong>
  </p>

  <h4>How to exploit the 86 textual properties of our data?</h4>

  <p>
    Our dataset includes 86 textual properties covering structural, lexical, emotional, and LIWC-derived indicators. 
    While these features were not originally designed specifically for toxicity detection, they collectively 
    capture many linguistic correlates of antagonistic or degrading speech. 
  </p>

  <div class="box-note box-collapsible">
  <div class="box-header">
    <div>
      <strong>Note:</strong> Learn more about what the 86 features capture:
    </div>
    <button class="box-toggle" aria-label="Toggle details"></button>
  </div>
  
  <div class="box-content">
    <p>
    <strong>Properties 1 to 18</strong> capture surface-level linguistic and structures of the text. They measure the basic composition 
    and readability of a message, including its length, complexity, and word or sentence structure. They include metrics such as 
    character and word counts, fractions of different character types (letters, digits, punctuation, etc.), sentence length averages, and 
    readability scores. Together, they provide a quantitative snapshot of how the text is written rather than what it expresses.
  </p>
  <p>
    <strong>Properties 19 to 21</strong> represent sentiment analysis metrics, derived using the VADER (Valence Aware Dictionary and 
    sEntiment Reasoner) model. They aim to measure the emotional polarity of the text. It includes the Positive Sentiment, Negative Sentiment 
    and Compound Sentiment. These features reflect the emotional valence of the language used.
  </p>
  <p>
    <strong>Properties 22 to 86</strong> are purely lexicon-based ratios, derived using a text analyis tool called Linguistic Inquiry and 
    Word Count (LIWC). Each LIWC represent the proportion of words in a text that belong to a certain lexicon: <code>LIWC_X = (nb words in lexicon X) 
    / (total nb words)</code>. While useful to see the presence of a certain vocubulary in a post, they remain purely lexical and doesn't treat emotion 
    or sentiment in any way.
  </p>
  </div>
  </div>

  <p> 
    Below are some metrics that are promising to use, based on our qualitative defintion of toxicity.
  </p> 



  <table class="toxicity-table">
    <thead>
      <tr>
        <th>Dimension</th>
        <th>Description</th>
        <th>Example of properties to use</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strongorange>Emotional intensity</strongorange></td>
        <td>Measures strength of negative emotion</td>
        <td>
          <code>Neg_Vader</code>, <code>LIWC_Negemo</code>,
          <code>LIWC_Anger</code>, <code>LIWC_Anx</code>…
        </td>
      </tr>
      <tr>
        <td><strongorange>Aggressive</strongorange></td>
        <td>Captures hostile or taboo language</td>
        <td>
          <code>LIWC_Swear</code>, <code>LIWC_Sexual</code>,
          <code>LIWC_Death</code>, <code>LIWC_Relig</code>…
        </td>
      </tr>
      <tr>
        <td><strongorange>Targeting</strongorange></td>
        <td>Focus on others (potential hostility toward people/groups)</td>
        <td>
          <code>LIWC_You</code>, <code>LIWC_Humans</code>,
          <code>LIWC_Social</code>…
        </td>
      </tr>
      <tr>
        <td><strongorange>Not cognitive</strongorange></td>
        <td>Opposite of reasoned or analytical tone</td>
        <td>
          <code>LIWC_Insight</code>, <code>LIWC_Cause</code>…
        </td>
      </tr>
      <tr>
        <td><strongorange>Violent and short</strongorange></td>
        <td>Style indicators of impulsive or aggressive speech</td>
        <td>
          <code>Number of UPPERCASE words</code>,
          <code>Readability index</code>…
        </td>
      </tr>
    </tbody>
  </table>

  <p>

  </p>
  <p>
    However, above all these textual properties, we also consider true <strong>negativity</strong> of a post to be a <strong>necessary</strong> 
    component for it to be toxic.
  </p> 

  <h4>Why is the VADER score not enough for measuring negativity?</h4>

  <p>
    The fundamental problem is that none of the 86 properties provides a reliable measure of <strong>emotional negativity</strong> in a post. 
    While the VADER scores seemed promising (optimized for social media texts and posts), they mainly work like a LIWC score 
    and calculate emotion of the <strong>language</strong> that was used (like an advanced lexical repertory of words that are conventionally 
    considered negative or harmful on social platforms). This cannot be considered an accurate measure of negativity in our context. 
    For example, some shortfalls of lexical metrics include:
  </p>
  <ul>
    <li><u>Sarcasm, irony, and context</u>: Commenting “Stupid guy” in a link targeting a funny video of a guy slipping or in a link targeting a video of 
        a political speech doesn't embody the same negativity.</li>
    <li><u>Cultural difference in language</u>: Some communities are (by culture) founded on “offensive” language without being negative.
        For example, a <em>CallOfDuty</em> subreddit will frequently mention terms like "kill" or “assassination”. 
        These words are not toxic in that community, but could be considered so in other subreddits (ones not related to war or gaming). </li>
  </ul>
  <p>
    Because toxicity in our definition must reflect true negative intent, not just negative words, lexical sentiment proves insufficient.
  </p>

  <h4>Good news: <code>LINK_SENTIMENT</code> helps!</h4>

  <p>
    <code>LINK_SENTIMENT</code> is derived using a supervised sentiment classifier trained on manually labeled Reddit data. 
    When labeling, the authors have taken into account their human judgement of context, irony etc. in addition of a purely lexical analysis. 
    So this binary sentiment classifier is designed specifically for inter-subreddit interactions and is our most reliable (categorical) 
    measure for pure negativity.
  </p>

  <p>
    To understand which properties meaningfully contribute to negative interactions, <strongorange> we compare normalized mean property values between positive and 
    negative links</strongorange>. Comparing group means detects systematic differences in linguistic patterns, while normalization accounts for differing scales 
    and distributions across features. Toxicity-related linguistic features are expected to cluster more strongly in negative posts. 
    Identifying features more common in negative interactions ensures alignment with our conceptual model (toxicity requires negativity)
  </p>
  <p>
  Although this procedure does not infer causality and therefore doesn't replace the qualitative choice of toxicity components, it strongly supports it 
  by ensuring that we later select variables that are consistent with the requirement of negativity in toxicity.
  </p>
  
  <div class="box-note box-collapsible">
  <div class="box-header">
    <div>
      <strong>Note:</strong> We select a z-difference threshold of 0.25, which retains ~20 properties. Why?
    </div>
    <button class="box-toggle" aria-label="Toggle details"></button>
  </div>
  
  <div class="box-content">
    <ul>
      <li>A difference of <strong>0.25 standard deviations</strong> corresponds to a small-to-moderate effect size. 
        In textual and psychological data, small-to-moderate effects are often meaningful because language patterns 
        are inherently noisy and multidimensional. A 0.25 cutoff ensures that <strong>weak or negligible features are excluded</strong>, 
        while <strong>features with clear group differences in positive vs negative sentiment are retained</strong>. This balances 
        strictness with comprehensiveness.</li>
      <li>As we explained above in our definition of toxicity, it cannot be captured by a single marker but arises from a 
        constellation of linguistic behaviors. Well-validated toxicity models (Google Perspective API, Jigsaw toxicity challenges...) 
        incorporate dozens of features, not just a few. On the other side, if the metric relied on too many features, it would 
        risk misinterpreting domain-specific vocabulary as toxic and producing high-variance estimates for minority communities. 
        Using ~20 features distributes the metric across a reasonably broad linguistic base, improving stability and most importantly 
        cross-subreddit generalizability.</li>
      <li>20 features is consistent with dimensionality in psycholinguistic instruments. Psycholinguistic tools like Empath (200+ categories) 
        or Biber's Dimensions of Register Variation (50+ linguistic dimensions) show that meaningful linguistic traits require between 10 and 
        30 variables to capture reliably.</li>
    </ul>
    
    <p>
      To summarize these arguments, retaining <strong>20 properties</strong> maximizes informational coverage, ensures robustness across heterogeneous subreddits, 
      matches standard dimensional expectations from psycholinguistics, and corresponds to a statistically meaningful effect-size cutoff. 
      The threshold of <strong>0.25</strong> therefore strikes the optimal balance between inclusiveness and discriminative power.
    </p>
  </div>
  </div>
  </p>
  The following figure highlights how textual properties differ between positive and negative links after z-score normalization, emphasizing features with the largest contrasts. 
  Hovering reveals the underlying property names and quantitative differences.
  </p>

  <iframe src="assets/interactives/property_diffs.html" width="100%" height = "500px" style="border: none;"></iframe>

  <h4>Combining Features Into the Toxicity Metric</h4>

  <p>
    Based on the conceptual qualitative choice of what defines toxicity and the empirical negativity analysis above, we select a subset of properties 
    to constitute our toxicity metric. These properties already exist in ratio form (0–1), ensuring comparability.
  </p>
  <p>
    We define <code>TOXICITY_SCORE</code> as a <strongorange>weighted linear combination of the selected properties</strongorange>. A linear model 
    is appropriate because it balances interpretability (and will allow immediate examination of feature contributions) with simple comparison across subreddits. 
    It also supports later scaling, standardization, and model-based refinements. Weights are based on informed judgments from literature on aggressive discourse 
    (e.g., strong weight on anger and swearing; negative weight on cognitive complexity features like insight or cause).
  </p>
  <p>
    <strong>Weights are normalized by the sum of absolute values.</strong> This choice preserves relative influence of features and rescales the score into a bounded and 
    interpretable range. Il also ensures no single feature arbitrarily dominates due to coefficient magnitude. This normalization is a standard approach for 
    interpretability when working with heterogeneous features and manually chosen weights.
  </p>

<p>
  <strong>Final weighted average of toxicity score</strong>
</p>
<p>
  This figure visualizes the relative importance of selected properties in the toxicity scoring; 
  slice size reflects coefficient magnitude while color encodes sign. 
  Hovering over the slices reveals the exact feature name and weight.
</p>

<div class="split-container">
  <!-- Partie gauche: tableau -->
  <div class="split-left">
    <table class="property-table">
      <thead>
        <tr>
          <th>Property</th>
          <th>Weight</th>
        </tr>
      </thead>
      <tbody>
        <!-- Positive weights (red), descending order -->
        <tr><td>NEGATIVE_SENTIMENT_CALCULATED_BY_VADER</td><td style="color:red;">5</td></tr>
        <tr><td>LIWC_SWEAR</td><td style="color:red;">5</td></tr>
        <tr><td>LIWC_ANGER</td><td style="color:red;">5</td></tr>
        <tr><td>FRACTION_OF_STOPWORDS</td><td style="color:red;">1</td></tr>
        <tr><td>LIWC_SOCIAL</td><td style="color:red;">1</td></tr>
        <tr><td>LIWC_HUMANS</td><td style="color:red;">1</td></tr>
        <tr><td>LIWC_NEGEMO</td><td style="color:red;">1</td></tr>
        <tr><td>LIWC_SEXUAL</td><td style="color:red;">1</td></tr>
        <!-- Negative weights (green) -->
        <tr><td>LIWC_CAUSE</td><td style="color:green;">-5</td></tr>
        <tr><td>LIWC_INSIGHT</td><td style="color:green;">-5</td></tr>
        <tr><td>FRACTION_OF_DIGITS</td><td style="color:green;">-3</td></tr>
      </tbody>
    </table>
  </div>
  
  <!-- Partie droite: camembert -->
  <div class="split-right">
    <iframe src="{{ '/assets/interactives/weights_slices.html' | relative_url }}" 
            class="interactive-plot">
    </iframe>
  </div>
</div>

  <p>
  Let's visualize our new metric's empirical distribution.
  </p>

  <img src="{{ '/assets/img/score_distrib.png' | relative_url }}" alt="score distrib" class="center large">

  <p>
  Theoretically, the score ranges from –1 to 1, but empirically falls within –0.1 to 0.4, reflecting real distributions in Reddit posts. 
  Since the scale is later standardized, this restricted range is not problematic.
  </p>
  
  
  
  <h3>Toxicity binary classification</h3>
  <p>
  We now have a reliable, continuous measure of toxicity. But it's not necessarily easy to interpret directly... 
  We would like to accompany this continuous metric with a binary classification: "Is this toxicity score value sufficiently high? 
  In other words, is this message toxic?"
  </p>

  <div class="box-warning">
    <code>TOXICITY_CAT</code>: Categorical metric that classifies posts as non-toxic (0) or toxic (1)
  </div>


  <p>
    This binary metric is obtaidned by passing a threshold on <code>TOXICITY_SCORE</code>. Specifically, the threshod above which messages 
    are flagged as toxic is <strong>the quantile-75 of the toxicity scores distribution</strong>. 
    That is, we mark as significantly toxic the top 25% messages with highest score.
  </p>

<div class="box-note box-collapsible">
  <div class="box-header">
    <div>
      <strong>Note:</strong>  Why is the quantile-75 of the distribution interpretable and consistent?
    </div>
    <button class="box-toggle" aria-label="Toggle details"></button>
  </div>
  
  <div class="box-content">
    <ul>
      <li>Toxic behavior is statistically rare — and tail-based thresholds reflect this. Online toxicity is considered to be heavy-tailed in 
        the sense that most posts are neutral, while toxic ones are relatively rare and concentrated at the upper end of the distribution. 
        Selecting the <strong>upper quartile (top 25%)</strong> isolates the <strong>long tail of intense language</strong> but ensures coverage of sufficiently 
        frequent abusive content. Thus, the 75th percentile is a natural and theoretically grounded boundary.</li>
      <li>75th percentile ensures class balance suitable for downstream modeling. Binary classification tasks require a <strong>non-trivial positive class</strong> to avoid degenerate models.  
        Cutting at 90% would lead to the positive class being too small (5–10%), sparse, with high variance. Cutting at 50% would lead to the positive class being too large, 
        not aligned with toxicity rarity.</li>
      <li>The 75th percentile is conceptually aligned with “significant toxicity”. By definition, we want to classify only <strong>strongly toxic</strong> posts as toxic. 
        Choosing the <strong>top quartile</strong> is a reliable way to identify posts with unusually high use of aggressive markers and atypical linguistic patterns. 
        This makes toxicity classification interpretable and consistent across subreddits.</li>
    </ul>
  </div>
  </div>

  <img src="{{ '/assets/img/metriccat.png' | relative_url }}" alt="cat before adj" class="center medium">

  <p>
    At first, it appears that the majority of toxic messages are actualy labeled positive, which is not desirable as we explicited that, 
    in our context, toxicity only comes with negativity. However, looking at the <strong>proportions</strong> of toxicity in each sentiment class (right barplot), 
    we see that around 78% of negative messages are labeled toxic, while only 19% of positive messages were. 
    Negative messages are much more likely to be toxic (which makes sense based on the properties we retained to define toxicity).
  </p>

  <p>
    To finalize our binary classification, we enforce the rule that only messages with negative sentiment (<code>LINK_SENTIMENT = −1</code>) can be classified 
    as toxic (</code>TOXICITY_CAT = 1</code>). This decision is based on a conceptual definition of toxicity as hostile, harmful, 
    or abusive discourse, which fundamentally requires a negative emotional valence. Although positive/neutral-sentiment messages may 
    sometimes contain high levels of features like swearing or anger, classifying them as toxic would introduce significant false positives that 
    do not align with the standard understanding of online harassment or abuse.
  </p>

  <p>
    <strong>Remark - </strong>The continuous <code>TOXICITY_SCORE</code> is preserved even for the links that are ultimately filtered out by the sentiment check. This score serves as a 
    crucial linguistic feature intensity metric, quantifying the sheer presence of aggressive or low-quality language features (e.g., swearing, anger, simplicity) 
    independent of the link's overall "true" sentiment. By keeping the raw score, we retain valuable information for later analysis, such as comparing the 
    distribution of strong language features across both toxic (negative sentiment) and non-toxic (positive sentiment) messages, which can reveal linguistic 
    patterns related to intense discourse beyond just negative intent.    
  </p>

  <div class="box-success">
    <strong>Great!</strong> We succesfully designed a reliable continuous metric to measure toxicity intentisy in a message, as well as 
    a binary classifier that distinguishes significantly toxic messages.
  </div>
  
  <p>
    We now use these two fundamental metrics to derive advcanded and case-specific indicators to later tackle specific tasks. 
  </p>

  <h2>Supporting and derived metrics</h2>

  <p>
    !! DEMANDER A CEDRIC SI VRAIMENT NECESSAIRE !! 
  </p>

</section>



<!-- #################################################################################################################################### -->
<!-- #################################################################################################################################### -->

<section id="resultats" class="section-block">
  <h2>Résultats</h2>
  
  <h3>Statistiques descriptives</h3>
  <p>
    Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur.
  </p>
  
  <h3>Découvertes principales</h3>
  <p>
    At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident.
  </p>
  
  <h3>Observations secondaires</h3>
  <p>
    Similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio.
  </p>
</section>

<section id="visualisation" class="section-block">
  <h2>Visualisation</h2>
  
  <h3>Graphiques de réseau</h3>
  <p>
    Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus.
  </p>
  
  <h3>Cartes de chaleur</h3>
  <p>
    Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae.
  </p>
  
  <h3>Analyses temporelles</h3>
  <p>
    Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.
  </p>
</section>
















<section id="discussion" class="section-block">
  <h2>Discussion</h2>
  
  <h3>Interprétation des résultats</h3>
  <p>
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
  </p>
  
  <h3>Comparaison avec la littérature</h3>
  <p>
    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
  </p>
  
  <h3>Implications pratiques</h3>
  <p>
    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
  </p>
</section>

<section id="limites" class="section-block">
  <h2>Limites</h2>
  
  <h3>Limitations méthodologiques</h3>
  <p>
    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
  </p>

  <div class="box-note">
    <strong>Note:</strong> Ceci est une box d'information avec du texte <em>en italique</em> et en <strong>gras</strong>.
  </div>

  <div class="box-warning">
    <strong>Remark:</strong> Les données présentées sont issues d'un échantillon limité de subreddits.
  </div>
  
  <div class="box-error">
    <strong>Warning:</strong> Cette analyse ne prend pas en compte les liens supprimés par les modérateurs.
  </div>
  
  <div class="box-success">
    <strong>Success:</strong> Le dataset a été collecté avec succès et contient plus de 100,000 liens.
  </div>
  
  <h3>Biais potentiels</h3>
  <p>
    Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium.
  </p>
  
  <h3>Pistes d'amélioration</h3>
  <p>
    Totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
  </p>
</section>















<section id="conclusion" class="section-block">
  <h2>Conclusion</h2>
  
  <h3>Synthèse des apports</h3>
  <p>
    Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
  </p>
  
  <h3>Perspectives futures</h3>
  <p>
    Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt.
  </p>
  
  <h3>Mot de fin</h3>
  <p>
    Ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam.
  </p>
</section>